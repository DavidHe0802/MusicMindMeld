{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"avN-JALETfTu"},"outputs":[],"source":["#Step 1: Prepare the DataFrame\n","#First, let's create a sample DataFrame with columns \"music_id\", \"music_name\", and \"music_description\".\n","import pandas as pd\n","\n","# data = {\n","#     \"music_id\": [1, 2, 3, 4, 5],\n","#     \"music_name\": [\"Song 1\", \"Song 2\", \"Song 3\", \"Song 4\", \"Song 5\"],\n","#     \"music_description\": [\n","#         \"A beautiful ballad with heartfelt lyrics.\",\n","#         \"An upbeat pop song that will make you dance.\",\n","#         \"A classic rock anthem with powerful guitar riffs.\",\n","#         \"A soulful R&B track with smooth vocals.\",\n","#         \"An electronic dance track with pulsing beats.\"\n","#     ]\n","# }\n","\n","# df = pd.DataFrame(data)\n","# print(df)"]},{"cell_type":"code","source":["data = pd.read_csv(\"/content/annotated_classical_music_dataset.csv\")\n","data.rename(columns={'name_movement': 'music_name', 'keywords': 'music_description','id': 'music_id'}, inplace=True)\n","df = pd.DataFrame(data.head(10))\n","print(df)"],"metadata":{"id":"aY1f7F2BlWlV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8 -q\n","!mkdir /root/.kaggle\n","\n","with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n","    f.write('{\"username\":\"hefugu\",\"key\":\"324ddef280041fb43790eb61df650409\"}')\n","\n","!chmod 600 /root/.kaggle/kaggle.json\n","\n","!kaggle datasets download -d imsparsh/musicnet-dataset\n","!unzip musicnet-dataset.zip"],"metadata":{"id":"L0pXIuoxkoWz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install farm-haystack[faiss]"],"metadata":{"id":"BDO8W8EDUb0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install faiss-cpu"],"metadata":{"id":"Z_TPbmn4VUN7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the FAISSDocumentStore\n","from haystack.document_stores import FAISSDocumentStore\n","\n","document_store = FAISSDocumentStore(\n","    faiss_index_factory_str=\"Flat\",  # Change this to your configuration needs\n","    sql_url=\"sqlite:///faiss_inde.db\",  # Specify your SQL URL here\n","    return_embedding=True\n",")\n","\n","# If needed, delete existing documents and embeddings\n","document_store.delete_all_documents()"],"metadata":{"id":"g3546qnxq43w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step 3: Convert DataFrame to Documents\n","#We need to convert the DataFrame rows into Haystack Document objects.\n","from haystack import Document\n","\n","documents = []\n","for _, row in df.iterrows():\n","    document = Document(\n","        content=row[\"music_description\"],\n","        meta={\"name\": row[\"music_name\"], \"id\": row[\"music_id\"]}\n","    )\n","    documents.append(document)"],"metadata":{"id":"r5hNeeQjUSH8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install farm-haystack[inference]"],"metadata":{"id":"DJwnYXrCWgcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step 4: Initialize the Retriever\n","#We'll initialize the DensePassageRetriever to encode and retrieve the most relevant music documents based on a given query.\n","from haystack.nodes import DensePassageRetriever\n","import sentence_transformers\n","\n","retriever = DensePassageRetriever(\n","    document_store=document_store,\n","    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n","    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n","    use_gpu=True,\n","    embed_title=True,\n",")"],"metadata":{"id":"L0qjNOgeUYe2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step 5: Write Documents to DocumentStore\n","#We'll delete any existing documents in the DocumentStore and write the new documents. Then, we'll update the embeddings using the retriever.\n","document_store.delete_documents()\n","document_store.write_documents(documents)\n","document_store.update_embeddings(retriever)"],"metadata":{"id":"YTutnro6Xjqf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step 6: Retrieve Relevant Documents\n","#Now, we can use the retriever to find the most relevant music documents based on a given query.\n","query = \"uplifting electronic music\"\n","\n","def retrieve_music_document(query):\n","    # Your RAG retrieval code here\n","    retrieved_docs = retriever.retrieve(query)\n","    return retrieved_docs[0]  # Return the most relevant document\n","retrieved_doc = retrieve_music_document(query)\n","print(retrieved_doc)"],"metadata":{"id":"0yGO13RjXoll"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Music Generation"],"metadata":{"id":"zA_sqYdUbKiN"}},{"cell_type":"code","source":["# !pip install --upgrade --quiet pip\n","# !pip install --upgrade --quiet transformers datasets[audio]"],"metadata":{"id":"xMGJYnrWhfie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from transformers import AutoProcessor, MusicgenForConditionalGeneration\n","from datasets import load_dataset\n","\n","# Load the MusicGen model and processor\n","processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n","model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n"],"metadata":{"id":"eki6kaSkhRuI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","# model.to(device);\n","# unconditional_inputs = model.get_unconditional_inputs(num_samples=1)\n","# audio_values = model.generate(**unconditional_inputs, do_sample=True, max_new_tokens=256)"],"metadata":{"id":"mmRhKBiUhsi3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Specify the folder containing the music files\n","music_folder = \"/content/musicnet/musicnet/train_data\"\n","query = \"uplifting electronic music\"\n","retrieved_doc = retrieve_music_document(query)"],"metadata":{"id":"jDAfqwSWhHDP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install librosa"],"metadata":{"id":"Q15n2rFJttRW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import librosa\n","# Get the music ID from the retrieved document\n","music_id = retrieved_doc.meta[\"id\"]\n","\n","# Construct the path to the music file\n","music_file = os.path.join(music_folder, f\"{music_id}.wav\")\n","\n","audio, sampling_rate = librosa.load(music_file, sr=None, duration=30)  # sr=None to preserve the original sampling rate\n"],"metadata":{"id":"G0hB6qR-hZ_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","# Load the music file using the datasets library\n","# dataset = load_dataset(\"audio\", data_files={\"audio\": music_file})\n","# sample = dataset[\"audio\"][0]\n","\n","music_tensor = torch.tensor(audio)\n","stride = len(music_tensor) // 96000\n","indices = torch.arange(0, len(music_tensor), stride)\n","music_tensor = music_tensor[indices]\n","\n","if len(music_tensor) > 96000:\n","    music_tensor = music_tensor[:96000]\n","\n","# Prepare the inputs for music generation\n","inputs = processor(\n","    audio=music_tensor,\n","    sampling_rate=32000,\n","    text=[query],\n","    padding=True,\n","    return_tensors=\"pt\",\n",")"],"metadata":{"id":"gU1suqirtlTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(music_file)"],"metadata":{"id":"Cm6tXrqrtHYK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate the music\n","audio_values = model.generate(**inputs, max_new_tokens=32)"],"metadata":{"id":"Cw8Y8DhihcQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Play or save the generated audio:\n","#To play the audio in a notebook:\n","from IPython.display import Audio\n","\n","sampling_rate = model.config.audio_encoder.sampling_rate\n","Audio(audio_values[0].numpy(), rate=sampling_rate)\n"],"metadata":{"id":"6qfkAXu3bmI_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#To save the audio as a WAV file\n","import scipy\n","\n","scipy.io.wavfile.write(\"musicgen_out.wav\", rate=sampling_rate, data=audio_values[0, 0].cpu().numpy())"],"metadata":{"id":"dNJ28EHlbrL1"},"execution_count":null,"outputs":[]}]}