# MusicMindMeld: Enhancing Text-Audio Generation by Music Classification and Retrieval-Augmented Generation

## Project Overview

This project aims to enhance the capabilities of music generation AI by integrating advanced deep learning techniques including genre classification and Retrieval-Augmented Generation (RAG). Our goal is to refine the process of generating music from textual descriptions, addressing common challenges such as alignment of the generated music with detailed text and consistency in longer compositions.

## Key Features

- **Advanced Genre Classification**: Utilizes Convolutional Neural Networks (CNNs) such as ResNet-50, GoogleNet, and VGG16 to classify music by genres accurately.
- **Retrieval-Augmented Generation**: Integrates a retrieval system that selects music pieces aligned with the user's textual input, enhancing the relevance and personalization of the generated music.
- **MUSICGEN Integration**: Employs the MUSICGEN model to produce new music pieces that inherit characteristics from both the retrieved music and the input text description.

## Code and Resources

The source code for this project is available on GitHub:
[MusicMindMeld Repository](https://github.com/DavidHe0802/MusicMindMeld)

## How It Works

**Music Classification**: The input music files are first processed through our trained CNNs to classify them into predefined genres.
**Retrieval Mechanism**: Based on the text input, the system retrieves a music piece that best matches the description from the pre-classified database.
**Music Generation**: The selected music piece and the text description are fed into MUSICGEN, which synthesizes a new music piece reflecting the input characteristics.

## Evaluation

We conducted a double-blind human study to evaluate the effectiveness of our system. The results indicated significant improvements in the fidelity and personalization of the music generated by our RAG-enhanced model compared to traditional methods.

## Future Works

**Integration with Advanced Generative Models:** Exploring the use of Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) to produce more nuanced musical outputs.
**User Interaction:** Implementing features that allow users to provide feedback and interact with the generation process to further refine the output.

## Authors

Runyu He
Bochen Wang
Junyi Zhu
Yixuan Yin

## Liscence

This project is licensed under the MIT License - see the LICENSE file for details.

